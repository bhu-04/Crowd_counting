# Crowd_counting
## CNN + Transformer Hybrid for Robust Crowd Counting

This project implements a **state-of-the-art crowd counting model** that fuses a **DenseNet-121 convolutional backbone** with **Multi-Scale Dilated Attention (MSDA)** and **Location-Enhanced Attention (LEA)** modules.  
Designed for datasets like **ShanghaiTech A**, this pipeline is flexible and can adapt to any crowd counting dataset in `.mat` format.

---

### ğŸ“ Repository Structure
```
Crowd_counting/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ images/     # Training images
â”‚   â”‚   â””â”€â”€ annots/     # .mat annotation files
â”‚   â””â”€â”€ test/
â”‚       â”œâ”€â”€ images/
â”‚       â””â”€â”€ annots/
â”œâ”€â”€ outputs/            # Model checkpoints & prediction visualizations
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model.py        # Model: DenseNet-121 + MSDA + LEA + regression head
â”‚   â”œâ”€â”€ train.py        # Training script (supports resume)
â”‚   â”œâ”€â”€ eval.py         # Evaluation & visualization
â”‚   â”œâ”€â”€ dataset.py      # Custom dataset loader
â”‚   â””â”€â”€ utils.py        # Checkpointing, plotting, etc.
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

### ğŸš€ Features
- âœ… DenseNet-121 backbone for strong local feature extraction  
- âœ… Transformer-inspired attention modules for global context  
- âœ… Robust regression head for precise crowd count prediction  
- âœ… Resume training seamlessly from the latest checkpoint  
- âœ… Visualization of predictions vs ground truth for every image  

---

### âš™ï¸ Installation

**1. Clone the repository**
```bash
git clone https://github.com/bhu-04/Crowd_counting.git
cd Crowd_counting
```

**2. Create and activate a virtual environment**
```bash
python -m venv venv
source venv/bin/activate   # macOS/Linux
# or
.env\Scriptsctivate    # Windows
```

**3. Install dependencies**
```bash
pip install -r requirements.txt
```

---

### ğŸ“‚ Dataset Access
The dataset used in this project is **not included** in the repository and is hosted securely on **Dropbox**.

To access the dataset:
1. Request access using this link: [Dropbox Dataset Link](https://www.dropbox.com/scl/fo/2lu0e2hyivphcr943as3m/AF44tvTT9URXmdxoZas_kJI?rlkey=9e9i5pdsas57cqwmweswspp9d&st=f6ml8wxc&dl=0)  
2. Once approved and downloaded, organize the dataset as follows:
   ```
   data/train/images/
   data/train/annots/
   data/test/images/
   data/test/annots/
   ```
3. Ensure annotation files are in `.mat` format for compatibility.  
If you donâ€™t have access, you can also use publicly available datasets like **ShanghaiTech Part A**.

---

### ğŸ§  Usage

#### ğŸ”¹ Train the Model
```bash
python src/train.py
```
- Automatically resumes from `outputs/best_model.pth` if available.  
- Model checkpoints saved after each epoch.

#### ğŸ”¹ Evaluate the Model
```bash
python src/eval.py
```
- Generates predicted vs. ground-truth visualizations.  
- Saves metrics (MAE, MSE) and images in `outputs/`.

---

### ğŸ§© Model Details
| Component | Description |
|------------|-------------|
| **Backbone** | DenseNet-121 pretrained on ImageNet |
| **Attention** | Multi-Scale Dilated Attention (MSDA) + Location-Enhanced Attention (LEA) |
| **Regressor** | Fully-connected layers with dropout for robustness |

---

### ğŸ§¹ Data Preprocessing
All images are resized to **384Ã—384** and normalized as per ImageNet standards:
```python
torchvision.transforms.Normalize([0.485, 0.456, 0.406],
                                 [0.229, 0.224, 0.225])
```
Crowd counts are extracted automatically from `.mat` annotation files.

---

### ğŸ§© Troubleshooting
| Issue | Possible Cause |
|--------|----------------|
| Model predicts near 0 for all images | Missing normalization in `dataset.py` |
| Resume checkpoint fails | Use the latest valid `.pth` file in `outputs/` |
| Prediction errors too large | Verify `.mat` key extraction and annotation parsing |

---

### ğŸ“¦ Requirements
```
torch >= 2.0.0
torchvision >= 0.15.0
numpy >= 1.22
matplotlib >= 3.5
scikit-learn >= 1.1
pillow >= 9.0
tqdm >= 4.60
scipy >= 1.7
```

---

### ğŸ¤ Contributing
Pull requests and issues are welcome!  
For questions or collaborations, contact **[bhu-04](https://github.com/bhu-04)**.

---
